---
title: "Random Projection Ensembles"
output: html_notebook
---
## Load Packages

```{r setup}
require("knitr")
opts_knit$set(root.dir = "/Users/pengqinzhi/Desktop/Random-Projection Ensembles Project")

library(parallel)
library(MASS)
library(class)
library(RPEnsemble)
library(dplyr)
library(tidyverse)
library(rpart)
library(readxl)
library(randomForest)

source("RProjEnsemble.R")
source("RProjGenerate.R")
source("RProjPredict.R")
```

## Real Data

```{r}
df.ionosphere <- read.table("data/ionosphere.data", sep = ",", header = FALSE, stringsAsFactors = TRUE)
df.ionosphere[,35] <- factor(df.ionosphere[,35], labels = c(1, 2))

df.hill <- do.call("rbind", lapply(c("data/Hill_Valley_without_noise_Training.data", "data/Hill_Valley_without_noise_Testing.data"), function(fn){read.table(fn, sep = ",", header = TRUE, stringsAsFactors = TRUE)}))
df.hill[,101] <- factor(df.hill[,101], labels = c(1, 2))

df.musk <- read.table("data/clean2.data", sep = ",", header = FALSE, stringsAsFactors = TRUE)
df.musk <- df.musk[, 3:ncol(df.musk)]
df.musk[,167] <- factor(df.musk[,167], labels = c(1, 2))

df.arrhythmia <- read.table("data/arrhythmia/arrhythmia.data", sep = ",",  header = FALSE, stringsAsFactors = TRUE)

df.gisette.X <- read.table("data/gisette/GISETTE/gisette_train.data",  header = FALSE, stringsAsFactors = TRUE)
df.gisette.y <- read.table("data/gisette/GISETTE/gisette_train.labels", sep = "\n",  header = FALSE, stringsAsFactors = TRUE)
df.gisette <- cbind(df.gisette.X,df.gisette.y)
df.gisette[,5001] <- factor(df.gisette[,5001], labels = c(1, 2))
```

### Test Code
#### Test 选择树
```{r}
d = 10
L = 10
clustertype = "Fork"
cores = 10
machines = NULL
seed =21
# create workers
if(clustertype == "Default"){cluster = makePSOCKcluster(1)}
if(clustertype == "Fork"){cluster = makeForkCluster(cores)}
if(clustertype == "Socket"){cluster = makePSOCKcluster(names = machines)}
# set the seed if have one
if(is.null(seed) == FALSE) {clusterSetRNGStream(cluster, seed)}

# do parallel to calculate each probability in 1:L
out <- parLapply(cl = cluster, 1:L, function(i){return(RProjPredict(XTrain, YTrain, XTest, d, projmethod = "Gaussian"))})

Train.error <- simplify2array(parLapply(cl = cluster, 1:L, function(i){return(mean(out[[i]][[1]] != YTrain))}))

indice <- which(Train.error <= quantile(Train.error, 0.75))
Test.prob <- parLapply(cl = cluster, indice, function(i){return(out[[i]][[2]])})

conf <- Reduce("+", Test.prob) / L 
conf
```

#### 测试不同方差的高斯投影

```{r}
dchoose <- rep(0,100)
err <- rep(0,100)
L = 1
d = 10
p=100
cluster = makePSOCKcluster(1)
clusterExport(cluster, ls(), envir = environment())
conf<- 0
conf2<-0

 # do parallel to calculate each probability in 1:L
for (i in 1:L){
    set.seed(i)
    p <- ncol(XTrain)   # the number of features
    n <- nrow(XTrain)
    ntest <- nrow(XTest)
    # generate a random matrix
    RP <- RProjGenerate(p, d, method = "Gaussian")
    
    # generate new projected matrices for training and test set
    XRPTrain <- as.matrix(crossprod(t(XTrain), RP),n,d)
    XRPTest <- as.matrix(crossprod(t(XTest), RP),ntest,d)
    
    # predict by decision tree
    out <- rpart(YTrain~., data.frame(XRPTrain))
    pred <- predict(out, data.frame(XRPTest))
    
    prob.class <- pred
    conf <- prob.class + conf
}
    
# sum probability matrices and select labels

Test.class <- colnames(conf)[max.col(conf)]
    
 for (i in 1:L){
    set.seed(i)
    p <- ncol(XTrain)   # the number of features
    n <- nrow(XTrain)
    ntest <- nrow(XTest)
    # generate a random matrix
    RP2 <- RProjGenerate(p, d, method = "sdnormal")
    
    # generate new projected matrices for training and test set
    XRPTrain <- as.matrix(crossprod(t(XTrain), RP2),n,d)
    XRPTest <- as.matrix(crossprod(t(XTest), RP2),ntest,d)
    
    # predict by decision tree
    out2 <- rpart(YTrain~., data.frame(XRPTrain))
    pred2 <- predict(out2, data.frame(XRPTest))
    
    
    prob.class2 <- pred2
    conf2 <- prob.class2 + conf2
}

# sum probability matrices and select labels
Test.class2 <- colnames(conf2)[max.col(conf2)]

sum(Test.class != Test.class2)

100*mean(Test.class != YTest)
100*mean(Test.class2 != YTest)

head(RP/RP2)

```


*在相同种子下，不同方差的高斯矩阵的元素之间是成比例缩放的。这意味着，相同种子下，不同方差的高斯投影实际上把数据投影到了同一低维空间。

```{r}
set.seed(1)
RP <- matrix(rnorm(10, 0, 1), 2, 5)
RP
set.seed(1)
RP2 <- matrix(rnorm(10, 0, 10), 2, 5)
RP2

```

#### Test doParallel
```{r}

d = 10
L = 10
clustertype = "Fork"
cores = 10
machines = NULL
seed =21
    # create workers
    if(clustertype == "Default"){cluster = makePSOCKcluster(1)}
    if(clustertype == "Fork"){cluster = makeForkCluster(cores)}
    if(clustertype == "Socket"){cluster = makePSOCKcluster(names = machines)}
    # set the seed if have one
    if(is.null(seed) == FALSE) {clusterSetRNGStream(cluster, seed)}
    
    # do parallel to calculate each probability in 1:L
    prob.class <- parLapply(cl = cluster, 1:L, function(i){return(RProjPredict(XTrain, YTrain, XTest, d, projmethod = "Gaussian"))})
    
    # sum probability matrices and select labels
    conf <- Reduce("+", prob.class) / L
    Test.class <- colnames(conf)[max.col(conf)]
    
    if(is.null(seed) == FALSE) {clusterSetRNGStream(cluster, seed)}
    # do parallel to calculate each probability in 1:L
    prob.class2 <- parLapply(cl = cluster, 1:L, function(i){return(RProjPredict(XTrain, YTrain, XTest, d, projmethod = "sdnormal"))})
    
    # sum probability matrices and select labels
    conf2 <- Reduce("+", prob.class2) / L
    Test.class2 <- colnames(conf2)[max.col(conf2)]

    sum(Test.class!=Test.class2)
    100*mean(Test.class!= YTest)
    100*mean(Test.class2!= YTest)
```


### choose the top trees
#### Gisette
p=5000

```{r}
df <- df.gisette

# set hyperparameters
L <- 500

# subsample the data
n.train <- 500
n.test <- nrow(df.ionosphere) - n.train
m <- ncol(df)
N_reps <- 20

d <- round(2 * log2(ncol(df)-1))

# initialize errors
errRProj.onezero <- rep(0, N_reps)

for (i in 1:N_reps){
    # training and test data set
    set.seed(123+i)
    s <- sample(nrow(df), n.train)
    XTrain <- as.matrix(df[s, -m])
    YTrain <- df[s, m]
    XTest <- as.matrix(df[-s, -m])
    YTest <- df[-s, m]
    
    # RProj 
    errRProj.onezero[i] <- 100*mean(as.factor(RProjEnsemble(XTrain, YTrain, XTest, L, d, projmethod = "onezero", clustertype = "Fork" , cores = 10, seed = 123+i)) != YTest)
}

mean(errRProj.onezero)
sd(errRProj.onezero)
```

50
> mean(errRProj.onezero)
[1] 10.42182
> sd(errRProj.onezero)
[1] 0.8494626

75
> mean(errRProj.onezero)
[1] 10.52455
> sd(errRProj.onezero)
[1] 0.890045

#### Musk
p=166

```{r}
df <- df.musk

# set hyperparameters
L <- 500

# subsample the data
n.train <- 500
n.test <- nrow(df.ionosphere) - n.train
m <- ncol(df)
N_reps <- 50

d <- c(2, 5, round(2 * log2(ncol(df)-1)), round(3*log2(ncol(df)-1)), round(4 * log2(ncol(df)-1)))

# initialize errors
errRProj.onezero <- rep(0, N_reps)

for (i in 1:N_reps){
    # training and test data set
    set.seed(123+i)
    s <- sample(nrow(df), n.train)
    XTrain <- as.matrix(df[s, -m])
    YTrain <- df[s, m]
    XTest <- as.matrix(df[-s, -m])
    YTest <- df[-s, m]
    
    # RProj 
    errRProj.onezero[i] <- 100*mean(as.factor(RProjEnsemble(XTrain, YTrain, XTest, L, d, projmethod = "onezero", clustertype = "Fork" , cores = 8, seed = 123+i)) != YTest)
}

mean(errRProj.onezero)
sd(errRProj.onezero)
```

L = 500
TOP 75%
> mean(errRProj.onezero)
[1] 8.716956
> sd(errRProj.onezero)
[1] 0.7159753


### different d & different random projections
#### Gisette
p = 5000

```{r}
df <- df.gisette
set.seed(12)
# subsample 1500 of 6000 observations
df <- df[sample(nrow(df), 1500),]

# set hyperparameters
L <- 100
d <- round(2 * log2(ncol(df)-1))

# subsample the data
n.train <- 1000
n.test <- nrow(df.ionosphere) - n.train
m <- ncol(df)
N_reps <- 50

# initialize errors
errRProj.Achlioptas <- rep(0, N_reps)
errRProj.Bernoulli <- rep(0, N_reps)
errRProj.Gaussian <- rep(0, N_reps)
errRProj.Haar <- rep(0, N_reps)
errRProj.axis <- rep(0, N_reps)
errRProj.onezero <- rep(0, N_reps)
errRProj.sdnormal <- rep(0, N_reps)

for (i in 1:N_reps){
    # training and test data set
    set.seed(123+i)
    s <- sample(nrow(df), n.train)
    XTrain <- as.matrix(df[s, -m])
    YTrain <- df[s, m]
    XTest <- as.matrix(df[-s, -m])
    YTest <- df[-s, m]
    
    # RProj 
    errRProj.onezero[i] <- 100*mean(as.factor(RProjEnsemble(XTrain, YTrain, XTest, L, d, projmethod = "onezero", clustertype = "Fork" , cores = 10, seed = 123+i)) != YTest)
}

mean(errRProj.Achlioptas)
sd(errRProj.Achlioptas)

mean(errRProj.Bernoulli)
sd(errRProj.Bernoulli)

mean(errRProj.Gaussian)
sd(errRProj.Gaussian)

mean(errRProj.sdnormal)
sd(errRProj.sdnormal)

mean(errRProj.Haar)
sd(errRProj.Haar)

mean(errRProj.axis)
sd(errRProj.axis)

mean(errRProj.onezero)
sd(errRProj.onezero)

```

L=100
n=1000
d = 2  5 25 37 49
> mean(errRProj.Achlioptas[1:41])
[1] 11.21463
> sd(errRProj.Achlioptas[1:41])
[1] 1.362637
> 
> mean(errRProj.Bernoulli[1:41])
[1] 11.14634
> sd(errRProj.Bernoulli[1:41])
[1] 1.442237
> 
> mean(errRProj.Gaussian[1:41])
[1] 11.46829
> sd(errRProj.Gaussian[1:41])
[1] 1.617164
> 
> mean(errRProj.sdnormal[1:41])
[1] 0
> sd(errRProj.sdnormal[1:41])
[1] 0
> 
> mean(errRProj.Haar[1:41])
[1] 11.07805
> sd(errRProj.Haar[1:41])
[1] 2.284241
> 
> mean(errRProj.axis[1:41])
[1] 9.112195
> sd(errRProj.axis[1:41])
[1] 1.90738
> 
> mean(errRProj.onezero[1:41])
[1] 23.02439
> sd(errRProj.onezero[1:41])
[1] 4.651493

#### Musk
p=166

d <- c(2, 5, round(2 * log2(ncol(df)-1)), round(3*log2(ncol(df)-1)), round(4 * log2(ncol(df)-1)))

```{r}
df <- df.musk

# set hyperparameters
L <- 100

# subsample the data
n.train <- 500
n.test <- nrow(df.ionosphere) - n.train
m <- ncol(df)
N_reps <- 50

d <- round(2 * log2(ncol(df)-1))

# initialize errors
errRProj.Achlioptas <- rep(0, N_reps)
errRProj.Bernoulli <- rep(0, N_reps)
errRProj.Gaussian <- rep(0, N_reps)
errRProj.Haar <- rep(0, N_reps)
errRProj.axis <- rep(0, N_reps)
errRProj.onezero <- rep(0, N_reps)
errRProj.sdnormal <- rep(0, N_reps)

for (i in 1:N_reps){
    # training and test data set
    set.seed(123+i)
    s <- sample(nrow(df), n.train)
    XTrain <- as.matrix(df[s, -m])
    YTrain <- df[s, m]
    XTest <- as.matrix(df[-s, -m])
    YTest <- df[-s, m]
    
    # RProj 
    errRProj.onezero[i] <- 100*mean(as.factor(RProjEnsemble(XTrain, YTrain, XTest, L, d, projmethod = "onezero", clustertype = "Fork" , cores = 10, seed = 123+i)) != YTest)
}

mean(errRProj.Achlioptas)
sd(errRProj.Achlioptas)

mean(errRProj.Bernoulli)
sd(errRProj.Bernoulli)

mean(errRProj.Gaussian)
sd(errRProj.Gaussian)

mean(errRProj.sdnormal)
sd(errRProj.sdnormal)

mean(errRProj.Haar)
sd(errRProj.Haar)

mean(errRProj.axis)
sd(errRProj.axis)

mean(errRProj.onezero)
sd(errRProj.onezero)
```

L = 100
n = 500
d = 15
> mean(errRProj.Achlioptas)
[1] 10.35651
> mean(errRProj.Bernoulli)
[1] 10.27288
> mean(errRProj.Gaussian)
[1] 10.32338
> mean(errRProj.sdnormal)
[1] 10.32338
> mean(errRProj.Haar)
[1] 10.33322
> mean(errRProj.axis)
[1] 8.935389
> mean(errRProj.onezero)
[1] 9.015743

d = 2  5 15 22 30
> mean(errRProj.Achlioptas)
[1] 11.92325
> sd(errRProj.Achlioptas)
[1] 1.167396
> mean(errRProj.Bernoulli)
[1] 11.99147
> sd(errRProj.Bernoulli)
[1] 1.165325
> mean(errRProj.Gaussian)
[1] 12.00098
> sd(errRProj.Gaussian)
[1] 1.070328
> mean(errRProj.sdnormal)
[1] 12.00131
> sd(errRProj.sdnormal)
[1] 1.070234
> mean(errRProj.Haar)
[1] 11.93342
> sd(errRProj.Haar)
[1] 1.031646
> mean(errRProj.axis)
[1] 9.453919
> sd(errRProj.axis)
[1] 0.9098035
> mean(errRProj.onezero)
[1] 11.37193
> sd(errRProj.onezero)
[1] 1.082881

d = 2  5  15  37  74  148
> mean(errRProj.Achlioptas)
[1] 10.7937
> mean(errRProj.Bernoulli)
[1] 10.97737
> mean(errRProj.Gaussian)
[1] 10.70351
> mean(errRProj.sdnormal)
[1] 10.70351
> mean(errRProj.Haar)
[1] 10.73795
> mean(errRProj.axis)
[1] 9.016071
> mean(errRProj.onezero)
[1] 9.337488

d = 2   5  15  37  59  74 111 133 148
> mean(errRProj.Achlioptas)
[1] 10.10364
> mean(errRProj.Bernoulli)
[1] 10.10462
> mean(errRProj.Gaussian)
[1] 10.07215
> mean(errRProj.sdnormal)
[1] 10.07215
> mean(errRProj.Haar)
[1] 10.03149
> mean(errRProj.axis)
[1] 8.823549
> mean(errRProj.onezero)
[1] 8.686127

随机采用d的方式整体比只选固定d的方式准确率高；对于部分数据集和特定的投影方式，可能有显著提升
采用高斯投影得到的准确率与高斯分布的方差无关

#### Hill
p=100

d = c(2, 5, 13, 33, 53, 66, 80)
```{r}
df <- df.hill

# set hyperparameters
L <- 100
d <- c(2, 5, round(2 * log2(ncol(df)-1)), round(3 * log2(ncol(df)-1)), round(4 * log2(ncol(df)-1)))

# subsample the data
n.train <- 500
n.test <- nrow(df.ionosphere) - n.train
m <- ncol(df)
N_reps <- 50

# initialize errors
errRProj.Achlioptas <- rep(0, N_reps)
errRProj.Gaussian <- rep(0, N_reps)
errRProj.Haar <- rep(0, N_reps)
errRProj.axis <- rep(0, N_reps)
errRProj.Bernoulli <- rep(0, N_reps)
errRProj.onezero <- rep(0, N_reps)
errRProj.sdnormal <- rep(0, N_reps)

for (i in 1:N_reps){
    # training and test data set
    set.seed(123+i)
    s <- sample(nrow(df), n.train)
    XTrain <- as.matrix(df[s, -m])
    YTrain <- df[s, m]
    XTest <- as.matrix(df[-s, -m])
    YTest <- df[-s, m]
    
    # RProj 
 
    errRProj.onezero[i] <- 100*mean(as.factor(RProjEnsemble(XTrain, YTrain, XTest, L, d, projmethod = "onezero", clustertype = "Fork" , cores = 10, seed = 123+i)) != YTest)

    
}

mean(errRProj.Achlioptas)
sd(errRProj.Achlioptas)
mean(errRProj.Bernoulli)
sd(errRProj.Bernoulli)
mean(errRProj.Gaussian)
sd(errRProj.Gaussian)
mean(errRProj.sdnormal)
sd(errRProj.sdnormal)
mean(errRProj.Haar)
sd(errRProj.Haar)
mean(errRProj.axis)
sd(errRProj.axis)
mean(errRProj.onezero)
sd(errRProj.onezero)
```

稀疏投影效果不好的原因可能是

L = 100
n = 500
d = 13
> mean(errRProj.Achlioptas)
[1] 30.77247
> mean(errRProj.Bernoulli)
[1] 30.81742
> mean(errRProj.Gaussian)
[1] 33.2809
> mean(errRProj.sdnormal)
[1] 33.2809
> mean(errRProj.Haar)
[1] 33.17135
> mean(errRProj.axis)
[1] 46.19944
> mean(errRProj.onezero)
[1] 46.79494


d = 2  5 13 20 27
> mean(errRProj.Achlioptas)
[1] 31.30618
> sd(errRProj.Achlioptas)
[1] 3.793276
> mean(errRProj.Bernoulli)
[1] 30.94663
> sd(errRProj.Bernoulli)
[1] 3.848941
> mean(errRProj.Gaussian)
[1] 33.72191
> sd(errRProj.Gaussian)
[1] 3.308617
> mean(errRProj.sdnormal)
[1] 33.72191
> sd(errRProj.sdnormal)
[1] 3.308617
> mean(errRProj.Haar)
[1] 33.4382
> sd(errRProj.Haar)
[1] 3.38911
> mean(errRProj.axis)
[1] 46.42697
> sd(errRProj.axis)
[1] 1.845506
> mean(errRProj.onezero)
[1] 48.65449
> sd(errRProj.onezero)
[1] 1.723909

d = 2 5 13 33 53 66 80
> mean(errRProj.Achlioptas)
[1] 13.71067
> mean(errRProj.Bernoulli)
[1] 12.24719
> mean(errRProj.Gaussian)
[1] 21.41854
> mean(errRProj.sdnormal)
[1] 21.41854
> mean(errRProj.Haar)
[1] 21.17697
> mean(errRProj.axis)
[1] 46.22472
> mean(errRProj.onezero)
[1] 46.52528

#### ionosphere
p=34

```{r}
df <- df.ionosphere

# set hyperparameters
L <- 100
d <-  c(2, 5, round(2 * log2(ncol(df)-1)), round(3 * log2(ncol(df)-1)), round(4 * log2(ncol(df)-1)))

# subsample the data
n.train <- 200
n.test <- nrow(df.ionosphere) - n.train
m <- ncol(df)
N_reps <- 50

# initialize errors
errRProj.Achlioptas <- rep(0, N_reps)
errRProj.Bernoulli <- rep(0, N_reps)
errRProj.Gaussian <- rep(0, N_reps)
errRProj.Haar <- rep(0, N_reps)
errRProj.axis <- rep(0, N_reps)
errRProj.onezero <- rep(0, N_reps)
errRProj.sdnormal <- rep(0, N_reps)

for (i in 1:N_reps){
    # training and test data set
    set.seed(123+i)
    s <- sample(nrow(df), n.train)
    XTrain <- as.matrix(df[s, -m])
    YTrain <- df[s, m]
    XTest <- as.matrix(df[-s, -m])
    YTest <- df[-s, m]
    
    # RProj 

    errRProj.onezero[i] <- 100*mean(as.factor(RProjEnsemble(XTrain, YTrain, XTest, L, d, projmethod = "onezero", clustertype = "Fork" , cores = 10, seed = 123+i)) != YTest)

    
}

mean(errRProj.Achlioptas)
sd(errRProj.Achlioptas)

mean(errRProj.Bernoulli)
sd(errRProj.Bernoulli)

mean(errRProj.Gaussian)
sd(errRProj.Gaussian)

mean(errRProj.sdnormal)
sd(errRProj.sdnormal)

mean(errRProj.Haar)
sd(errRProj.Haar)

mean(errRProj.axis)
sd(errRProj.axis)

mean(errRProj.onezero)
sd(errRProj.onezero)
```
L = 100
n = 200
d = 10
> mean(errRProj.Achlioptas)
[1] 5.86755
> mean(errRProj.Bernoulli)
[1] 5.880795
> mean(errRProj.Gaussian)
[1] 5.880795
> mean(errRProj.sdnormal)
[1] 5.880795
> mean(errRProj.Haar)
[1] 5.854305
> mean(errRProj.axis)
[1] 7.152318
> mean(errRProj.onezero)
[1] 9.668874

d = 2  5 10 15 20
> mean(errRProj.Achlioptas)
[1] 5.89404
> sd(errRProj.Achlioptas)
[1] 1.944636
> mean(errRProj.Bernoulli)
[1] 6.013245
> sd(errRProj.Bernoulli)
[1] 1.862781
> mean(errRProj.Gaussian)
[1] 6.119205
> sd(errRProj.Gaussian)
[1] 2.155554
> mean(errRProj.sdnormal)
[1] 6.119205
> sd(errRProj.sdnormal)
[1] 2.155554
> mean(errRProj.Haar)
[1] 6.039735
> sd(errRProj.Haar)
[1] 2.147567
> mean(errRProj.axis)
[1] 7.298013
> sd(errRProj.axis)
[1] 1.646766
> mean(errRProj.onezero)
[1] 7.523179
> sd(errRProj.onezero)
[1] 2.289264

d = 2  5 10 15 20 25 31
> mean(errRProj.Achlioptas)
[1] 5.788079
> mean(errRProj.Bernoulli)
[1] 5.94702
> mean(errRProj.Gaussian)
[1] 5.761589
> mean(errRProj.sdnormal)
[1] 5.761589
> mean(errRProj.Haar)
[1] 5.92053
> mean(errRProj.axis)
[1] 7.324503
> mean(errRProj.onezero)
[1] 8.953642


### different projections
#### Musk
p=166
```{r}
df <- df.musk

# set hyperparameters
L <- 100
d <- round(2 * log2(ncol(df)-1))

# subsample the data
n.train <- 500
n.test <- nrow(df.ionosphere) - n.train
m <- ncol(df)
N_reps <- 50

# initialize errors
errRProj.Achlioptas <- rep(0, N_reps)
errRProj.Gaussian <- rep(0, N_reps)
errRProj.Haar <- rep(0, N_reps)
errRProj.axis <- rep(0, N_reps)
errRProj.Bernoulli <- rep(0, N_reps)
errRProj.onezero <- rep(0, N_reps)
errRProj.sdnormal <- rep(0, N_reps)

for (i in 1:N_reps){
    # training and test data set
    set.seed(123+i)
    s <- sample(nrow(df), n.train)
    XTrain <- as.matrix(df[s, -m])
    YTrain <- df[s, m]
    XTest <- as.matrix(df[-s, -m])
    YTest <- df[-s, m]
    
    # RProj 
    errRProj.onezero[i] <- 100*mean(as.factor(RProjEnsemble(XTrain, YTrain, XTest, L, d, projmethod = "onezero", clustertype = "Fork" , cores = 10, seed = 123+i)) != YTest)
    errRProj.sdnormal[i] <- 100*mean(as.factor(RProjEnsemble(XTrain, YTrain, XTest, L, d, projmethod = "sdnormal", clustertype = "Fork" , cores = 10, seed = 123+i)) != YTest)
    
}

mean(errRProj.onezero)
mean(errRProj.sdnormal)
```

L = 10
n = 500
> mean(errRProj.Achlioptas)
[1] 10.54116
> mean(errRProj.Bernoulli)
[1] 10.66317
> mean(errRProj.Gaussian)
[1] 10.67727
> mean(errRProj.sdnormal)
[1] 10.69957
> mean(errRProj.Haar)
[1] 10.62971
> mean(errRProj.axis)
[1] 9.161364
> mean(errRProj.onezero)
[1] 9.553624


L = 100
n = 500
> mean(errRProj.Achlioptas)
[1] 10.29977
> mean(errRProj.Bernoulli)
[1] 10.27845
> mean(errRProj.Gaussian)
[1] 10.30699 
> mean(errRProj.sdnormal)
[1] 10.32338
> mean(errRProj.Haar)
[1] 10.39718
> mean(errRProj.axis)
[1] 8.929157
> mean(errRProj.onezero)
[1] 9.015743



高纬度下，axis效果更好
训练集n太小时，RP可能会把所有数据分为一类

#### ionosphere
p=34
```{r}
df <- df.ionosphere
# set hyperparameters
d <- round(2 * log2(ncol(df)-1))
L <- 100

# subsample the data
n.train <- 200
n.test <- nrow(df) - n.train
N_reps <- 50

# initialize errors
errRProj.Achlioptas <- rep(0, N_reps)
errRProj.Gaussian <- rep(0, N_reps)
errRProj.Haar <- rep(0, N_reps)
errRProj.axis <- rep(0, N_reps)
errRProj.Bernoulli <- rep(0, N_reps)
errRProj.onezero<- rep(0, N_reps)
errRProj.sdnormal<- rep(0, N_reps)

for (i in 1:N_reps){
    set.seed(123+i)
    # training and test data set
    s <- sample(nrow(df), n.train)
    XTrain <- as.matrix(df[s, -35])
    YTrain <- df[s, 35]
    XTest <- as.matrix(df[-s, -35])
    YTest <- df[-s, 35]
    
    # RProj 
    errRProj.onezero[i] <- 100*mean(as.factor(RProjEnsemble(XTrain, YTrain, XTest, L, d, projmethod = "onezero", clustertype = "Fork" , cores = 10, seed = 123+i)) != YTest)
    errRProj.sdnormal[i] <- 100*mean(as.factor(RProjEnsemble(XTrain, YTrain, XTest, L, d, projmethod = "sdnormal", clustertype = "Fork" , cores = 10, seed = 123+i)) != YTest)
    
}


```
L = 10           L = 100
n = 50           n = 50
[1] 19.36877    [1] 16.69767
[1] 18.66445    [1] 17.45515
[1] 19.34884    [1] 17.69435
[1] 19.69435    [1] 17.21595
[1] 15.88704    [1] 14.63123

n = 100           n = 100
[1] 12.36653    [1] 9.442231
[1] 12.14343    [1] 9.36255
[1] 11.96016    [1] 9.816733
[1] 12.86853    [1] 9.498008
[1] 11.11554    [1] 9.434263

n = 200           n = 200
[1] 8.092715    [1] 5.735099
[1] 8.304636    [1] 5.774834
[1] 8.092715    [1] 5.84106
[1] 8.13245     [1] 5.788079
[1] 8.331126    [1] 7.086093

                > mean(errRProj.onezero)
                [1] 9.668874
                > mean(errRProj.sdnormal)
                [1] 5.880795


训练集数量增多，ensemble的个数增多时，效果更好
训练集较少时，axis投影方法更好

#### Hill
p=100
```{r}
df <- df.hill

# set hyperparameters
L <- 10
d <- round(2 * log2(ncol(df)-1))

# subsample the data
n.train <- 500
n.test <- nrow(df.ionosphere) - n.train
m <- ncol(df)
N_reps <- 50

# initialize errors
errRProj.Achlioptas <- rep(0, N_reps)
errRProj.Gaussian <- rep(0, N_reps)
errRProj.Haar <- rep(0, N_reps)
errRProj.axis <- rep(0, N_reps)
errRProj.Bernoulli <- rep(0, N_reps)
errRProj.onezero <- rep(0, N_reps)
errRProj.sdnormal <- rep(0, N_reps)

for (i in 1:N_reps){
    # training and test data set
    set.seed(123+i)
    s <- sample(nrow(df), n.train)
    XTrain <- as.matrix(df[s, -m])
    YTrain <- df[s, m]
    XTest <- as.matrix(df[-s, -m])
    YTest <- df[-s, m]
    
    # RProj 
    errRProj.onezero[i] <- 100*mean(as.factor(RProjEnsemble(XTrain, YTrain, XTest, L, d, projmethod = "onezero", clustertype = "Fork" , cores = 10, seed = 123+i)) != YTest)
    errRProj.sdnormal[i] <- 100*mean(as.factor(RProjEnsemble(XTrain, YTrain, XTest, L, d, projmethod = "sdnormal", clustertype = "Fork" , cores = 10, seed = 123+i)) != YTest)
    
}

mean(errRProj.onezero)
mean(errRProj.sdnormal)
```

L = 10
n = 100
[1] 43.69604
[1] 43.5
[1] 45.39388
[1] 45.25719
[1] 49.43885

n = 500
[1] 34.71348
[1] 33.92697
[1] 35.46067
[1] 35.79213
[1] 46.41011
> mean(errRProj.onezero)
[1] 47.0618
> mean(errRProj.sdnormal)
[1] 35.95506


```{r}
    # RProj 
    errRProj.Achlioptas[i] <- 100*mean(as.factor(RProjEnsemble(XTrain, YTrain, XTest, L, d, projmethod = "Achlioptas", clustertype = "Fork" , cores = 10, seed = 123+i)) != YTest)
    errRProj.Bernoulli[i] <- 100*mean(as.factor(RProjEnsemble(XTrain, YTrain, XTest, L, d, projmethod = "Bernoulli", clustertype = "Fork" , cores = 10, seed = 123+i)) != YTest)
    errRProj.Gaussian[i] <- 100*mean(as.factor(RProjEnsemble(XTrain, YTrain, XTest, L, d, projmethod = "Gaussian", clustertype = "Fork" , cores = 10, seed = 123+i)) != YTest)
    errRProj.Haar[i] <- 100*mean(as.factor(RProjEnsemble(XTrain, YTrain, XTest, L, d, projmethod = "Haar", clustertype = "Fork" , cores = 10, seed = 123+i)) != YTest)
    errRProj.axis[i] <- 100*mean(as.factor(RProjEnsemble(XTrain, YTrain, XTest, L, d, projmethod = "axis", clustertype = "Fork" , cores = 10, seed = 123+i)) != YTest)
```
