---
title: "Homework 2"
author: "Qinzhi Peng"
date: 'Assigned: March 21, 2022'
output: 
  html_document:
    toc: true
    toc_depth: 3
    theme: cerulean
    highlight: tango
---

##### This homework is due by **11:59PM Eastern Time on Wednesday, March 30**.  

##### To complete this assignment, follow these steps:

1. Download the `homework2.Rmd` file from Canvas or the course website.

2. Open `homework2.Rmd` in RStudio.

3. Replace the "Your Name Here" text in the `author:` field with your own name.

4. Supply your solutions to the homework by editing `homework2.Rmd`.

5. When you have completed the homework and have **checked** that your code both runs in the Console and knits correctly when you click `Knit HTML`, rename the R Markdown file to `homework2_YourNameHere.Rmd`, and submit both the `.Rmd` file and the `.html` output file on Canvas  (YourNameHere should be changed to your own name.)

### Preamble: Loading packages, bikes data

```{r}
library(ggplot2)
library(plyr)
library(ISLR)
library(MASS)
library(knitr)
library(splines)
library(gam)

cbPalette <- c("#999999", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")

options(scipen = 4)

# Load bikes data
bikes <- read.csv("http://www.andrew.cmu.edu/user/achoulde/95791/data/bikes.csv", header = TRUE)

# Transform temp and atemp to degrees C instead of [0,1] scale
# Transform humidity to %
# Transform wind speed (multiply by 67, the normalizing value)

bikes <- transform(bikes,
                   temp = 47 * temp - 8,
                   atemp = 66 * atemp - 16,
                   hum = 100 * hum,
                   windspeed = 67 * windspeed)

# The mapvalues() command from the plyr library allows us to easily
# rename values in our variables.  Below we use this command to change season
# from numeric codings to season names.

bikes <- transform(bikes, 
                   season = mapvalues(season, c(1,2,3,4), 
                                      c("Winter", "Spring", "Summer", "Fall")))

```

### Problem 1 [7 points]: Placing knots, choosing degrees of freedom

> This question is intended to provide you with practice on manual knot placement, and to improve your understanding of effective degrees of freedom selection for smoothing splines.

> The following command loads a data frame called `spline.data` into your workspace.  This question gets you to analyse `spline.data`.  

```{r}
con <- url("http://www.andrew.cmu.edu/user/achoulde/95791/data/splines.Rdata")
load(con)
close(con)

```

##### **(a)** Use `qplot` to plot the data, and use `stat_smooth` to overlay a cubic spline with 8 degrees of freedom.  

```{r}
# Edit me
qplot(data = spline.data, x = x, y = y) + stat_smooth(method = "lm", formula = y ~ bs(x, df = 8))
```

##### **(b)** The following command forms the basis functions that get used by the `lm` command to fit a cubic spline with 8 degrees of freedom.  Explore this object that is constructed to figure out how many knots are placed, and where the knots are located.  How many knots are placed?  Where are they placed?

```{r}
basis.obj <- with(spline.data, bs(x, 8))
# Edit me
attr(basis.obj,"knots")
```

- **Your answer here.**
There are five knots. And they are placed in 1.708333,3.366667,5.025000,6.683333,8.341667 respectively.

##### **(c)** Instead of specifying the degrees of freedom to the `bs()` function, now try manually selecting knots.  You should supply a `knots` vector containing 5 values.  Try to pick the knots as optimally as possible.  Use `qplot` and `stat_smooth` to show a plot of the data with the cubic spline with your choice of knots overlaid.  Explain your choice of knot location.

```{r}
# Edit me
qplot(data = spline.data, x = x, y = y) + stat_smooth(method = "lm", formula = y ~ bs(x, df = 8, knots = c(5.5, 6.5, 7.5, 8.5, 9.5)))
```

- **Your answer here.**
From 0 to 5.5, data have a smaller change than the data when x > 5.5. And 6.5, 7.5, 8.5, 9.5 these four points seem to be the points where data change the trend.

##### **(d)** Use the `lm` function to fit two models:  One using the model from part (a), and another using the model you settled on in part (c).  Compare the R-squared values.  Which model better fits the data?

```{r}
# Edit me
lm.fit <- lm(y ~ bs(x, df = 8), data = spline.data)
lm.fit2 <- lm(y ~ bs(x, df = 8, knots = c(5.5, 6.5, 7.5, 8.5, 9.5)), data = spline.data)

summary(lm.fit)
summary(lm.fit2)
```

- **Your answer here.**
The R-squared value from part (a) is 0.3233 and The R-squared value from part (c) is 0.8903. Obviously, the model in part (c) better fits the data.

##### **(e)** Use the `smooth.spline` command with `cv = TRUE` to fit a smoothing spline to the data.  What degrees of freedom does the CV routine select for the smoothing spline?  How does this compare to the degrees of freedom of your model from part (c)?

```{r}
# Edit me
fit <- with(spline.data, smooth.spline(x, y, cv=TRUE))
fit
```

- **Your answer here.**
The degrees of freedom is 28.82183, which is much bigger than the degrees of freedom of the model from part (c).

##### **(f)** Use the `smooth.spline` command with `cv = TRUE` to fit a smoothing spline to the first half of the data (x <= 5.0).  What degrees of freedom does the CV routine select for this smoothing spline?

```{r}
# Edit me
subset <- subset(spline.data, x <= 5.0)
fit2 <- with(subset, smooth.spline(x, y, cv=TRUE))
fit2
```
- **Your answer here.**
The degrees of freedom for this smoothing spline is 8.354332.

##### **(g)** Repeat part (f), this time fitting the smoothing spline on just the second half of the data (`x` > 5.0).  How does the optimal choice for the second half of the data compare to the optimal choice for the first half.  Are they very different?  Can you explain what's happening?

```{r}
# Edit me
subset2 <- subset(spline.data, x > 5.0)
fit3 <- with(subset2, smooth.spline(x, y, cv=TRUE))
fit3
```

- **Your answer here.**
The optimal choices are very different. The smoothing parameter for the second half of the data is 0.6032928, and the lambda is 0.00003350187. Both are smaller than the first half. The degrees of freedom for the second half is 18.5047. That is much bigger than the first half, which means the second model is more flexiable than the first one.  That's make sense, since the data(`x` > 5.0) change rapidly compared to the data(`x` <= 5.0), we need place more knots for the data(`x` > 5.0).
    
### Problem 2 [13 points]: Cross-validation

> This problem asks you to code up your own cross-validation routine that will produce $K$-fold CV error estimates for polynomial regression, regression splines, and smoothing splines.

> You should code up a function called `smoothCV` that takes the following inputs.

**Inputs**:

| Argument | Description                                           | 
|----------|-------------------------------------------------------|
|  `x`     | a vector giving the values of a predictor variable    |
|  `y`     | a vector giving the values of the response variable   |
|  `K`     | the number of folds to use in the validation routine  |
| `df.min` | the smallest number of degrees of freedom to consider |
| `df.max` | the largest number of degrees of freedom to consider  |

> `smoothCV` should return the following output

**Output**:

Your function should return a `data.frame` object giving the $K$-fold error estimates for: polynomial regression, cubic splines, and smoothing splines, with the degrees of freedom ranging from `df.min` to `df.max`.  The data frame should have three columns:  `df`, `method`, `error`.  

**Sample output:**  

```
 df           method cv.error
  1             poly     25.4
  1     cubic.spline       NA
  1 smoothing.spline       NA
  2             poly     21.1
  2     cubic.spline       NA
  2 smoothing.spline     20.0
  3             poly     15.2
  3     cubic.spline     15.2
  3 smoothing.spline     16.1
```

**Note**: In the example above, we had `df.min = 1` and `df.max = 3`.  We saw in lecture that a cubic spline with $K$ interior knots has $K+3$ degrees of freedom.  Thus we cannot form a cubic spline with `df` of 1 or 2.  Similarly, the `smooth.spline()` fitting function in **R** requires that `df` > 1.  **If the given method cannot be fit at the specified degrees of freedom, you should report the cv.error as NA, as shown above.**

**Note**: When $n$ is not divisible by $K$, it will not be possible to partition the sample into $K$ *equally sized groups*.  You should make the groups as equally sized as possible.  When the groups are of unequal size, the preferred way of calculating the average MSE is by using a *weighted* average.  More precisely, if $n_k$ is the number of observations in fold $k$ and $MSE_k$ is the MSE estimated from fold $k$, the weighted average estimate of MSE is:

$$ CV_{K} = \sum_{k = 1}^K \frac{n_k}{n} MSE_k $$

It's easy to check that if $n$ is evenly divisible by $K$ then each $n_k = n/K$, and so the above expression reduces to the formula you saw in class: $CV_{K} = \frac{1}{K}\sum_{k = 1}^K MSE_k$

##### **(a)** [5 points] Code up `smoothCV()` according to the specification above.  A function header is provided for you to get you started.

```{r}
smoothCV <- function(x, y, K = 10, df.min = 1, df.max = 10) {
  # Your code here
  # partition the sample
  set.seed(1)
  datasize <- length(x)
  num <- round(datasize/K)
  indexes <- 1:datasize
  folds <- list()
   
  for (i in 1:K) {
    if (i != K) {
      folds[[i]] <- sort(sample(indexes, num))
      for (j in 1:num) {
        indexes <- indexes[-which(indexes==folds[[i]][j])]
      }
    } else {
      folds[[i]] <- sort(indexes)
    }
  }
  
  # fit the models
  data1 <- do.call(rbind, Map(data.frame, x = x, y = y))
  df.num <- df.max - df.min + 1
  cv.poly <- rep(0, df.max)
  cv.bs <- rep(0, df.max)
  cv.s <- rep(0, df.max)
  
  for (m in df.min:df.max) {
    mse.poly <- rep(0, K)
    mse.bs <- rep(0, K)
    mse.s <- rep(0, K)
    
    for (i in 1:K) {
      train <- data1[-folds[[i]],]
      valid <- data1[folds[[i]],]
      valid.y <- y[folds[[i]]]
      valid.x <- x[folds[[i]]]
      
      #fit.poly <- lm(y ~ poly(x, m), data = data1, subset = train)
      #mse.poly[i] <- mean(summary((fit.poly)$residuals^2))
      fit.poly <- lm(y ~ poly(x, m), data = train)
      mse.poly[i] <- mean((valid.y - predict(fit.poly, valid))^2)
      
      if (m >= 3) {
        fit.bs <- lm(y ~ bs(x, df = m), data = train)
        mse.bs[i] <- mean((valid.y - predict(fit.bs, valid))^2)
      }

      if (m != 1) {
        fit.s <- with(train, smooth.spline(x, y, df = m))
        mse.s[i] <- mean((valid.y - predict(fit.s, valid.x)$y)^2)
      }
    }

    # calculating the weighted average MSE
    weight <- lengths(folds)/sum(lengths(folds))
    cv.poly[m] <- weighted.mean(mse.poly, w = weight)
    cv.bs[m] <- weighted.mean(mse.bs, w = weight)
    cv.s[m] <- weighted.mean(mse.s, w = weight)
  }
  
  # form a data.frame
  df = rep(df.min:df.max, each = 3)
  method = rep(c("poly", "cubic.spline", "smoothing.spline"), times = df.num)
  cv.error = rep(0, df.num*3)
  for (n in 1:df.num) {
      cv.error[3*(n-1)+1] <- cv.poly[df.min + n - 1]
      
      if (cv.bs[df.min + n - 1] == 0) {
        cv.error[3*(n-1)+2] <- "NA"
      } else {
        cv.error[3*(n-1)+2] <- cv.bs[df.min + n - 1]
      }
      
      if (cv.s[df.min + n - 1] == 0) {
        cv.error[3*(n-1)+3] <- "NA"
      } else {
        cv.error[3*(n-1)+3] <- cv.s[df.min + n - 1]
      }
  }
  cv.df <- data.frame(df, method, cv.error)

  return(cv.df) 
  
}

```

##### **(b)** [2 points] Write a function for plotting the results of `smoothCV()`.  

**Inputs**: 

| Argument         | Description                                                      | 
|------------------|------------------------------------------------------------------|
| `smoothcv.err`   | a data frame obtained by running the `smoothCV` function         |
| `K`              | the number of folds used in the CV routine                       |
| `title.text`     | the desired title for the plot                                   |
| `y.scale.factor` | if provided, a relative upper bound on the upper y-axis limit    |

**Additional details**

- `smoothcv.err`: This data frame has the exact structure of the `smoothCV()` output illustrated in the preamble of this problem.  
- `y.scale.factor`: You can use the `is.null(y.scale.factor)` command to test if the user provided a value of `y.scale.factor`.  If this value is non-null, you should set the y-axis limits of your plot to (`lower`, `upper`), where `lower` is the *smallest CV error of any method for any choice of* `df`, and `upper` is `y.scale.factor * lower`.  With `ggplot2` graphics, you may use syntax such as `p + ylim(100, 2 * 100)` to set the y-axis limits to `(100, 200)`.  

**Output**: For the example above if we had `K = 5`, the plot would look something like this:

![sample cv plot](http://andrew.cmu.edu/~achoulde/95791/misc/smooth_cv_plot.png)



```{r}
plot.smoothCV <- function(smoothcv.err, K, title.text = "", y.scale.factor = NULL) {
  # Your code here
  df.min <- min(smoothcv.err$df)
  df.max <- max(smoothcv.err$df)
  df1 <- df.min:df.max
  if (df.min == 1) {
    df2 <- df.min-2:df.max
    df3 <- df.min-1:df.max
  } else if (df.min ==2) {
    df2 <- df.min-1:df.max
    df3 <- df.min:df.max
  } else {
    df2 <- df.min:df.max
    df3 <- df.min:df.max
  }
  df.num <- df.max - df.min + 1
  
  num <- nrow(smoothcv.err)
  cv.min <- as.double(smoothcv.err$cv.error[1])
  cv.max <- as.double(smoothcv.err$cv.error[1])

  for (i in 2:num) {
    if (smoothcv.err$cv.error[i] != "NA") {
      if (as.double(smoothcv.err$cv.error[i]) < cv.min) {
        cv.min = as.double(smoothcv.err$cv.error[i])
         
      }
      
      if (as.double(smoothcv.err$cv.error[i]) > cv.max) {
        cv.max = as.double(smoothcv.err$cv.error[i])
      }
    }
  }  

  cv.min <- as.numeric(cv.min)
  cv.max <- as.numeric(cv.max)
    
  
  cv.poly <- rep(0, df.max)
  cv.bs <- rep(0, df.max)
  cv.s <- rep(0, df.max)
  for (n in 1:df.num) {
    cv.poly[df.min + n - 1] <- smoothcv.err$cv.error[3*(n-1)+1]
    
    if (smoothcv.err$cv.error[3*(n-1)+2] == "NA"){
      cv.bs[df.min + n - 1] <- ""
    } else {
      cv.bs[df.min + n - 1]  <- smoothcv.err$cv.error[3*(n-1)+2]
    }
    
    if (smoothcv.err$cv.error[3*(n-1)+3]  == "NA"){
      cv.s[df.min + n - 1] <- ""
    } else {
      cv.s[df.min + n - 1] <- smoothcv.err$cv.error[3*(n-1)+3]
    }
  }
  
  data <- data.frame(do.call("cbind", list(df, cv.poly, cv.bs, cv.s)))

  print(data)
  
  if (is.null(y.scale.factor)) {
    ggplot(data = data, aes(x=df1)) + geom_point(aes(x = df1, y=cv.poly, color="cv.poly")) + geom_line(aes(y = cv.poly), color="red") + geom_point(aes(x = df2, y=cv.bs, color="cv.bs")) + geom_line(aes(y = cv.bs), color="green") + geom_point(aes(x = df, y=cv.s, color="cv.s")) + geom_line(aes(y = cv.s), color="blue")  + xlab("Degree of Freedom") + ylab(title.text) 
  }

}
plot.smoothCV(fit,K = 10,title.text = "10-Knots mnth cv error")

```

##### **(c)** [3 points]  You saw the `bikes` data on Homework 1.  Use your `smoothCV` function with 10-fold cross-validation to determine the best choice of model and degrees of freedom for modeling the relationship between `cnt` and each of these inputs: `mnth`, `atemp`, `hum`, and `windspeed`.  Rely on your `plot.smoothCV` plotting routine to support your choice of model for each of the inputs.  

**Hint:** Use the `y.scale.factor` argument of your `plot.smoothCV` function wisely.  If you see that a particular model's error starts to blow up as `df` increases, you should set `y.scale.factor` appropriately to prevent the extremely large error estimates from misleading you in your assessment of which model to use.

```{r}
# Edit me
fit1 <- smoothCV(bikes$mnth, bikes$cnt)
fit2 <- smoothCV(bikes$atemp, bikes$cnt)
fit3 <- smoothCV(bikes$hum, bikes$cnt)
fit4 <- smoothCV(bikes$windspeed, bikes$cnt)

fit1
which.min(fit1$cv.error)
plot.smoothCV(fit1,K = 10,title.text = "10-Knots mnth cv error")
fit2
which.min(fit2$cv.error)
plot.smoothCV(fit2,K = 10,title.text = "10-Knots atemp cv error")
fit3
which.min(fit3$cv.error)
plot.smoothCV(fit2,K = 10,title.text = "10-Knots hum cv error")
fit4
which.min(fit4$cv.error)
plot.smoothCV(fit2,K = 10,title.text = "10-Knots windspeed cv error")
```

- **Your answer here**
According to the model selection, for `mnth`, I choose smoothing.spline model and the degrees of freedom is 6, which has the smallest cv error; for `atemp`, I choose poly model and df is 3, which has a small cv error and is simple; for `hum`, I choose smoothing.spline model and df is 5, which has the smallest cv error; for `windspeed`, I choose smoothing.spline model and df is 3, which has the smallest cv error.

##### **(d)** Use the `gam` library and the models you selected in part (c) to fit an additive model of `cnt` on `mnth`, `atemp`, `hum` and `windspeed`.  Use the `plot` command on your fitted `glm` object with the arguments `se = TRUE, col = 'steelblue', lwd = 3` to produce plots of the fitted curves. (See `?plot.gam` for details.)

```{r}
# Edit me
gam.m1 <- gam(cnt ~ s(mnth, 6), data=bikes)
gam.m2 <- gam(cnt ~ poly(atemp, 3), data=bikes)
gam.m3 <- gam(cnt ~ s(hum, 5), data=bikes)
gam.m4 <- gam(cnt ~ s(windspeed, 3), data=bikes)
# Ensure that all 4 model fits appear in the same figure
par(mfrow = c(1,4))
# Write your plot() command below this comment
plot(gam.m1, se = TRUE, col = 'steelblue', lwd = 3)
plot(gam.m2, se = TRUE, col = 'steelblue', lwd = 3)
plot(gam.m3, se = TRUE, col = 'steelblue', lwd = 3)
plot(gam.m4, se = TRUE, col = 'steelblue', lwd = 3)
```

##### **(e)** Use your model from part **(d)** to calculate the "% deviance explained" by your model.  

> The "% deviance explained" is the Generalized Additive Model analog of R-squared.  It is exactly equal to the R-squared for regression models that can be fit with both the `gam` and `lm` functions.  If you have a fitted `gam` model called, say, `gam.fake`, you can calculate the % deviance explained with the following syntax:

```{r, eval = FALSE}
1 - gam.fake$deviance / gam.fake$null.deviance
```

```{r}
# Edit me
deviance1 <- 1 - gam.m1$deviance / gam.m1$null.deviance
deviance2 <- 1 - gam.m2$deviance / gam.m2$null.deviance
deviance3 <- 1 - gam.m3$deviance / gam.m3$null.deviance
deviance4 <- 1 - gam.m4$deviance / gam.m4$null.deviance

deviance1
deviance2
deviance3
deviance4
```

##### **(f)** Compare the % deviance explained of your Additive Model to the R-squared from running a linear regression of `cnt` on the same input variables.  Does the Additive Model considerably outperform the linear regression model?

```{r}
# Edit me
lm.fit1 <- lm(cnt ~ mnth, data = bikes)
lm.fit2 <- lm(cnt ~ atemp, data = bikes)
lm.fit3 <- lm(cnt ~ hum, data = bikes)
lm.fit4 <- lm(cnt ~ windspeed, data = bikes)

summary(lm.fit1)
summary(lm.fit2)
summary(lm.fit3)
summary(lm.fit4)
```

- **Your answer here**
For `mnth`, the R-squared from the linear model is 0.07839, which is much smaller than the % deviance explained(which is 0.3887454). That means this Additive Model considerably outperforms the linear regression model.

For `atemp`, the R-squared from the linear model is 0.3982, which is smaller than the % deviance explained(which is 0.4648683). That means this Additive Model is better than the linear regression model.

For `hum`, the R-squared from the linear model is 0.01013, which is smaller than the % deviance explained(which is 0.08856061). That means this Additive Model is better than the linear regression model. But they are both terrible.

For `windspeed`, the R-squared from the linear model is 0.05501, which is smaller than the % deviance explained(which is 0.06078646). That means this Additive Model is better than the linear regression model. But they are both terrible.

